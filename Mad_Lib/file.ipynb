{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import everygrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.lm import MLE\n",
    "from nltk.util import everygrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # convert tet to lowercase\n",
    "    text = text.lower()\n",
    "    # remove all punctuations\n",
    "    text = re.sub(f'[^\\w\\s]', '', text)\n",
    "    stop_words = set(stopwords.words())\n",
    "    text = [i for i in word_tokenize(text) if i not in stop_words]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk installs\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language model\n",
    "def train(text_data):\n",
    "    token  = [word_tokenize(i) for i in text_data]\n",
    "    token = [word for sublist in token for word in sublist]\n",
    "    bigrams = list(everygrams(token, max_len=2))\n",
    "\n",
    "    if bigrams:\n",
    "        train = MLE(2)\n",
    "        # bigram_model.fit(bigrams)\n",
    "        # return bigram_model\n",
    "\n",
    "        vocab = nltk.lm.Vocabulary(token)\n",
    "        train.fit([bigrams], vocab)\n",
    "        return train\n",
    "    \n",
    "    else:\n",
    "        print(\"No bigrams found\")\n",
    "        return None\n",
    "\n",
    "text_data = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"She sells sea shells by the sea shore\",\n",
    "    \"The cat in the hat comes back\",\n",
    "    \"To be or not to be that is the question\",\n",
    "    \"All is well that ends well\",\n",
    "]\n",
    "\n",
    "model = train(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate  mad libs\n",
    "def generate_mad_libs(template, model):\n",
    "    placeholders = re.findall(r'(\\{.*?\\})', template)\n",
    "    mad_libs = template\n",
    "\n",
    "    for i in placeholders:\n",
    "        if i == \"noun\":\n",
    "            replacement = input(\"Enter a noun: \")\n",
    "        elif i == \"verb\":\n",
    "            replacement = input(\"Enter a verb: \")\n",
    "        elif i == \"adjective\":\n",
    "            replacement = input(\"Enter an adjective: \")\n",
    "        elif i == \"adverb\":\n",
    "            replacement = input(\"Enter an adverb: \")\n",
    "        else:\n",
    "            replacement = input(\"Enter a word: \")\n",
    "\n",
    "        mad_libs = mad_libs.replace(f\"{{{i}}}\", replacement, 1)\n",
    "    return mad_libs\n",
    "    template = \"The {adjective} {noun} {adverb} {verb} over the {adjective} {noun}\"\n",
    "    finished_mad_libs = generate_mad_libs(template, model)\n",
    "\n",
    "    print(finished_mad_libs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
